{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpfDdmiM+1CsjiI88hStTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuzetao/vue/blob/main/%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%A7%A3%E6%96%B9%E7%A8%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kUvIm3L_KCA",
        "outputId": "e647d506-9ecb-44e9-9ce5-3df2da75b88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第0次迭代损失=-536.3048706054688\n",
            "第100次迭代损失=-1402.6988525390625\n",
            "第200次迭代损失=-1718.354736328125\n",
            "第300次迭代损失=-1716.08984375\n",
            "第400次迭代损失=-1731.32958984375\n",
            "第500次迭代损失=-1812.880126953125\n",
            "第600次迭代损失=-1865.522216796875\n",
            "第700次迭代损失=-1573.2799072265625\n",
            "第800次迭代损失=-1808.927490234375\n",
            "第900次迭代损失=-1723.392822265625\n",
            "第1000次迭代损失=-1769.7742919921875\n",
            "第1100次迭代损失=-1763.021484375\n",
            "第1200次迭代损失=-1594.899169921875\n",
            "第1300次迭代损失=-1565.9656982421875\n",
            "第1400次迭代损失=-1747.092529296875\n",
            "第1500次迭代损失=-1696.3145751953125\n",
            "第1600次迭代损失=-1721.678955078125\n",
            "第1700次迭代损失=-1800.42626953125\n",
            "第1800次迭代损失=-1864.531005859375\n",
            "第1900次迭代损失=-1610.739990234375\n",
            "第2000次迭代损失=-1920.5665283203125\n",
            "第2100次迭代损失=-1829.51806640625\n",
            "第2200次迭代损失=-1794.820556640625\n",
            "第2300次迭代损失=-1788.2144775390625\n",
            "第2400次迭代损失=-1703.732177734375\n",
            "第2500次迭代损失=-1661.0953369140625\n",
            "第2600次迭代损失=-1834.802490234375\n",
            "第2700次迭代损失=-1732.309814453125\n",
            "第2800次迭代损失=-1788.378173828125\n",
            "第2900次迭代损失=-1658.6171875\n",
            "第3000次迭代损失=-1733.75634765625\n",
            "第3100次迭代损失=-1824.2410888671875\n",
            "第3200次迭代损失=-1719.896728515625\n",
            "第3300次迭代损失=-1709.0673828125\n",
            "第3400次迭代损失=-1855.9638671875\n",
            "第3500次迭代损失=-1755.8973388671875\n",
            "第3600次迭代损失=-1708.902099609375\n",
            "第3700次迭代损失=-1849.1285400390625\n",
            "第3800次迭代损失=-1607.573974609375\n",
            "第3900次迭代损失=-1850.43701171875\n",
            "第4000次迭代损失=-1726.2001953125\n",
            "第4100次迭代损失=-1718.23486328125\n",
            "第4200次迭代损失=-1771.953857421875\n",
            "第4300次迭代损失=-1733.4150390625\n",
            "第4400次迭代损失=-1623.098388671875\n",
            "第4500次迭代损失=-1759.8787841796875\n",
            "第4600次迭代损失=-1609.5889892578125\n",
            "第4700次迭代损失=-1743.2900390625\n",
            "第4800次迭代损失=-1919.759033203125\n",
            "第4900次迭代损失=-1676.237060546875\n",
            "第5000次迭代损失=-1774.67236328125\n",
            "第5100次迭代损失=-1713.576416015625\n",
            "第5200次迭代损失=-1920.1678466796875\n",
            "第5300次迭代损失=-1905.589111328125\n",
            "第5400次迭代损失=-1744.4393310546875\n",
            "第5500次迭代损失=-1634.00927734375\n",
            "第5600次迭代损失=-1795.6998291015625\n",
            "第5700次迭代损失=-1890.253662109375\n",
            "第5800次迭代损失=-1768.239013671875\n",
            "第5900次迭代损失=-1747.6551513671875\n",
            "第6000次迭代损失=-1813.9088134765625\n",
            "第6100次迭代损失=-1718.3304443359375\n",
            "第6200次迭代损失=-1827.771240234375\n",
            "第6300次迭代损失=-1818.56103515625\n",
            "第6400次迭代损失=-1788.080322265625\n",
            "第6500次迭代损失=-1711.535400390625\n",
            "第6600次迭代损失=-1709.27490234375\n",
            "第6700次迭代损失=-1593.341064453125\n",
            "第6800次迭代损失=-1634.630859375\n",
            "第6900次迭代损失=-1663.677001953125\n",
            "第7000次迭代损失=-1609.9561767578125\n",
            "第7100次迭代损失=-1696.7650146484375\n",
            "第7200次迭代损失=-1764.254150390625\n",
            "第7300次迭代损失=-1789.786865234375\n",
            "第7400次迭代损失=-1820.611083984375\n",
            "第7500次迭代损失=-1681.1094970703125\n",
            "第7600次迭代损失=-1773.2392578125\n",
            "第7700次迭代损失=-1654.96337890625\n",
            "第7800次迭代损失=-1690.65380859375\n",
            "第7900次迭代损失=-1844.828857421875\n",
            "第8000次迭代损失=-1663.9503173828125\n",
            "第8100次迭代损失=-1849.057373046875\n",
            "第8200次迭代损失=-1875.092529296875\n",
            "第8300次迭代损失=-1591.5780029296875\n",
            "第8400次迭代损失=-1829.90966796875\n",
            "第8500次迭代损失=-1643.7479248046875\n",
            "第8600次迭代损失=-1642.60546875\n",
            "第8700次迭代损失=-1738.983154296875\n",
            "第8800次迭代损失=-1749.6524658203125\n",
            "第8900次迭代损失=-1778.8330078125\n",
            "第9000次迭代损失=-1699.560302734375\n",
            "第9100次迭代损失=-1704.3994140625\n",
            "第9200次迭代损失=-1669.585205078125\n",
            "第9300次迭代损失=-1773.949951171875\n",
            "第9400次迭代损失=-1647.3564453125\n",
            "第9500次迭代损失=-1720.39990234375\n",
            "第9600次迭代损失=-1916.1317138671875\n",
            "第9700次迭代损失=-1684.02294921875\n",
            "第9800次迭代损失=-1759.5281982421875\n",
            "第9900次迭代损失=-1847.9337158203125\n",
            "第10000次迭代损失=-1752.11767578125\n",
            "第10100次迭代损失=-1608.6177978515625\n",
            "第10200次迭代损失=-1782.1788330078125\n",
            "第10300次迭代损失=-1939.543701171875\n",
            "第10400次迭代损失=-1669.104248046875\n",
            "第10500次迭代损失=-1753.47216796875\n",
            "第10600次迭代损失=-1874.947265625\n",
            "第10700次迭代损失=-1864.2357177734375\n",
            "第10800次迭代损失=-1779.4422607421875\n",
            "第10900次迭代损失=-1745.1763916015625\n",
            "第11000次迭代损失=-1824.735595703125\n",
            "第11100次迭代损失=-1813.6806640625\n",
            "第11200次迭代损失=-1678.6380615234375\n",
            "第11300次迭代损失=-1847.37451171875\n",
            "第11400次迭代损失=-1752.4384765625\n",
            "第11500次迭代损失=-1902.3792724609375\n",
            "第11600次迭代损失=-1538.8773193359375\n",
            "第11700次迭代损失=-1779.193115234375\n",
            "第11800次迭代损失=-1662.9425048828125\n",
            "第11900次迭代损失=-1687.0025634765625\n",
            "第12000次迭代损失=-1676.8809814453125\n",
            "第12100次迭代损失=-1790.3480224609375\n",
            "第12200次迭代损失=-1687.008544921875\n",
            "第12300次迭代损失=-1878.0172119140625\n",
            "第12400次迭代损失=-1703.224853515625\n",
            "第12500次迭代损失=-1901.9891357421875\n",
            "第12600次迭代损失=-1961.635009765625\n",
            "第12700次迭代损失=-1737.1500244140625\n",
            "第12800次迭代损失=-1907.0643310546875\n",
            "第12900次迭代损失=-1744.9935302734375\n",
            "第13000次迭代损失=-1706.0556640625\n",
            "第13100次迭代损失=-2027.830078125\n",
            "第13200次迭代损失=-1674.41357421875\n",
            "第13300次迭代损失=-1625.862060546875\n",
            "第13400次迭代损失=-1804.722900390625\n",
            "第13500次迭代损失=-1670.6304931640625\n",
            "第13600次迭代损失=-1897.666259765625\n",
            "第13700次迭代损失=-1760.370361328125\n",
            "第13800次迭代损失=-1601.9840087890625\n",
            "第13900次迭代损失=-1820.17333984375\n",
            "第14000次迭代损失=-1516.944580078125\n",
            "第14100次迭代损失=-1813.850830078125\n",
            "第14200次迭代损失=-1771.0968017578125\n",
            "第14300次迭代损失=-1693.7392578125\n",
            "第14400次迭代损失=-1659.552734375\n",
            "第14500次迭代损失=-1665.1483154296875\n",
            "第14600次迭代损失=-1763.995849609375\n",
            "第14700次迭代损失=-1680.43701171875\n",
            "第14800次迭代损失=-1931.97265625\n",
            "第14900次迭代损失=-1682.161865234375\n",
            "第15000次迭代损失=-1639.984375\n",
            "第15100次迭代损失=-1821.8087158203125\n",
            "第15200次迭代损失=-1750.8486328125\n",
            "第15300次迭代损失=-1682.557861328125\n",
            "第15400次迭代损失=-1716.64306640625\n",
            "第15500次迭代损失=-1752.7227783203125\n",
            "第15600次迭代损失=-1733.2694091796875\n",
            "第15700次迭代损失=-1582.2772216796875\n",
            "第15800次迭代损失=-1673.1650390625\n",
            "第15900次迭代损失=-2051.487060546875\n",
            "第16000次迭代损失=-1728.872314453125\n",
            "第16100次迭代损失=-1652.142822265625\n",
            "第16200次迭代损失=-1968.0703125\n",
            "第16300次迭代损失=-1641.11669921875\n",
            "第16400次迭代损失=-1707.671630859375\n",
            "第16500次迭代损失=-1654.107421875\n",
            "第16600次迭代损失=-1706.0853271484375\n",
            "第16700次迭代损失=-1652.6005859375\n",
            "第16800次迭代损失=-1654.99951171875\n",
            "第16900次迭代损失=-1680.9344482421875\n",
            "第17000次迭代损失=-1761.339111328125\n",
            "第17100次迭代损失=-1809.316162109375\n",
            "第17200次迭代损失=-1763.861572265625\n",
            "第17300次迭代损失=-1973.4619140625\n",
            "第17400次迭代损失=-1750.9239501953125\n",
            "第17500次迭代损失=-1681.82177734375\n",
            "第17600次迭代损失=-1677.117919921875\n",
            "第17700次迭代损失=-1754.28369140625\n",
            "第17800次迭代损失=-1761.7445068359375\n",
            "第17900次迭代损失=-1667.510009765625\n",
            "第18000次迭代损失=-1827.537841796875\n",
            "第18100次迭代损失=-1694.807373046875\n",
            "第18200次迭代损失=-1783.93798828125\n",
            "第18300次迭代损失=-1651.41015625\n",
            "第18400次迭代损失=-1664.59228515625\n",
            "第18500次迭代损失=-1667.8453369140625\n",
            "第18600次迭代损失=-1757.9609375\n",
            "第18700次迭代损失=-1777.9769287109375\n",
            "第18800次迭代损失=-1741.4613037109375\n",
            "第18900次迭代损失=-1841.496826171875\n",
            "第19000次迭代损失=-1731.32763671875\n",
            "第19100次迭代损失=-1625.7373046875\n",
            "第19200次迭代损失=-1860.837890625\n",
            "第19300次迭代损失=-1906.7890625\n",
            "第19400次迭代损失=-1809.584716796875\n",
            "第19500次迭代损失=-1832.292236328125\n",
            "第19600次迭代损失=-1806.07421875\n",
            "第19700次迭代损失=-1769.975830078125\n",
            "第19800次迭代损失=-1823.3533935546875\n",
            "第19900次迭代损失=-1690.171875\n",
            "测试输入:tensor([[-1.1019,  1.7295,  0.7822],\n",
            "        [-0.1501,  0.3772,  1.4433]], device='cuda:0')\n",
            "人工神经网络计算结果:tensor([[1.6685],\n",
            "        [1.7375]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "g的值:tensor([1.7330, 2.2075], device='cuda:0', grad_fn=<SubBackward0>)\n",
            "\n",
            "理论最优值:tensor([[1.6707],\n",
            "        [1.7301]], device='cuda:0')\n",
            "g的值:tensor([1.7330, 2.2075], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import Linear, ReLU, Sequential\n",
        "\n",
        "net= Sequential(\n",
        "    Linear(3,8),\n",
        "    ReLU(),\n",
        "    Linear(8,16),\n",
        "    ReLU(),\n",
        "    Linear(16,32),\n",
        "    ReLU(),\n",
        "    Linear(32,16),\n",
        "    ReLU(),\n",
        "    Linear(16,8),\n",
        "    ReLU(),\n",
        "    \n",
        "    Linear(8,1),\n",
        "\n",
        "\n",
        ")\n",
        "def g(x,y):\n",
        "    x0,x1,x2=x[:,0]**0,x[:,1]**1,x[:,2]**2\n",
        "    y0=y[:,0]\n",
        "    return (x0+x1+x2)*y0-y0*y0-x0*x1*x2\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "\n",
        "optimizer =Adam(net.parameters())\n",
        "for step in range(20000):\n",
        "    optimizer.zero_grad()\n",
        "    x=torch.randn(1000,3).cuda()\n",
        "    net.cuda()\n",
        "    y=net(x)\n",
        "    outputs=g(x,y).cuda()\n",
        "    loss=-torch.sum(outputs)\n",
        "    #loss=loss-torch.sum(outputs)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 100 ==0:\n",
        "        print('第{}次迭代损失={}'.format(step,loss))\n",
        "\n",
        "x_test =torch.randn(2,3).cuda()\n",
        "#x_test =torch.ones(2,3).cuda()\n",
        "print('测试输入:{}'.format(x_test))\n",
        "\n",
        "net.cuda()\n",
        "y_test=net(x_test)\n",
        "print('人工神经网络计算结果:{}'.format(y_test))\n",
        "print('g的值:{}'.format(g(x_test,y_test)))\n",
        "print('')\n",
        "\n",
        "def argmax_g(x):\n",
        "    x0,x1,x2=x[:,0]**0,x[:,1]**1,x[:,2]**2\n",
        "    return 0.5*(x0+x1+x2)[:,None]\n",
        "yref_test=argmax_g(x_test)\n",
        "print('理论最优值:{}'.format(yref_test))\n",
        "print('g的值:{}'.format(g(x_test,yref_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nBOfCXXAmas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test =torch.randn(2,3).cuda()\n",
        "#x_test =torch.ones(2,3).cuda()\n",
        "print('测试输入:{}'.format(x_test))\n",
        "\n",
        "net.cuda()\n",
        "y_test=net(x_test)\n",
        "print('人工神经网络计算结果:{}'.format(y_test))\n",
        "print('g的值:{}'.format(g(x_test,y_test)))\n",
        "print('')\n",
        "\n",
        "def argmax_g(x):\n",
        "    x0,x1,x2=x[:,0]**0,x[:,1]**1,x[:,2]**2\n",
        "    return 0.5*(x0+x1+x2)[:,None]\n",
        "yref_test=argmax_g(x_test)\n",
        "print('理论最优值:{}'.format(yref_test))\n",
        "print('g的值:{}'.format(g(x_test,yref_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRT-o9HvAaz3",
        "outputId": "19b33751-8c88-4c10-f2d8-ffe1ada90a40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "测试输入:tensor([[ 0.6088, -0.6107,  0.1332],\n",
            "        [ 0.2219,  0.1880, -1.1451]], device='cuda:0')\n",
            "人工神经网络计算结果:tensor([[0.2033],\n",
            "        [1.2531]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "g的值:tensor([0.0523, 1.3150], device='cuda:0', grad_fn=<SubBackward0>)\n",
            "\n",
            "理论最优值:tensor([[0.2035],\n",
            "        [1.2496]], device='cuda:0')\n",
            "g的值:tensor([0.0523, 1.3150], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}